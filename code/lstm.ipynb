{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import see_rnn\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_dict(d):\n",
    "    return {v:k for k,v in iter(d.items())}\n",
    "\n",
    "\n",
    "def load_lm_dataset(fname):\n",
    "    sents = []\n",
    "    cnt = 0\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            if cnt == 0:\n",
    "                cnt += 1\n",
    "                continue\n",
    "            items = line.strip().split('\\t')\n",
    "            sents.append(items[0].split())\n",
    "    return sents\n",
    "\n",
    "\n",
    "def load_np_dataset(fname):\n",
    "    sents = []\n",
    "    cnt = 0\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            if cnt == 0:\n",
    "                cnt += 1\n",
    "                continue\n",
    "            items = line.strip().split('\\t')\n",
    "            verb_idx = int(items[2])\n",
    "            verb_pos = items[3]\n",
    "            sent = [verb_pos] + items[0].split()[:verb_idx]\n",
    "            sents.append(sent)\n",
    "    return sents\n",
    "\n",
    "\n",
    "def load_lm_np_dataset(fname):\n",
    "    sents = []\n",
    "    cnt = 0\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            if cnt == 0:\n",
    "                cnt += 1\n",
    "                continue\n",
    "            items = line.strip().split('\\t')\n",
    "            verb_idx = int(items[2])\n",
    "            verb = items[4]\n",
    "            inf_verb = items[5]\n",
    "            sent = items[0].split()[:verb_idx] + [verb, inf_verb]\n",
    "            sents.append(sent)\n",
    "    return sents\n",
    "\n",
    "\n",
    "def pad_sequence(seq, left=1, right=1):\n",
    "    return left*[\"<s>\"] + seq + right*[\"</s>\"]\n",
    "\n",
    "\n",
    "# For RNN\n",
    "# just convert each sentence to a list of indices\n",
    "# after padding each with <s> ... </s> tokens\n",
    "def seq_to_indices(words, word_to_num):\n",
    "    return np.array([word_to_num[w] for w in words])\n",
    "\n",
    "\n",
    "def docs_to_indices(sents, word_to_num, pad_left=1, pad_right=1):\n",
    "    sents = [pad_sequence(s, pad_left, pad_right) for s in sents]\n",
    "    sents_idx = []\n",
    "    for sent in sents:\n",
    "        words = [w if w in word_to_num else 'UNK' for w in sent]\n",
    "        sents_idx.append(seq_to_indices(words, word_to_num))\n",
    "\n",
    "    # return as numpy array for fancier slicing\n",
    "    return np.array(sents_idx, dtype=object)\n",
    "\n",
    "\n",
    "def offset_seq(seq):\n",
    "    return seq[:-1], seq[1:]\n",
    "\n",
    "\n",
    "def offset_np(seq):\n",
    "    return seq[1:], [seq[0]]\n",
    "\n",
    "\n",
    "def offset_lm_np(seq):\n",
    "    return seq[:-2], [seq[-2], seq[-1]]\n",
    "\n",
    "\n",
    "def seqs_to_lmXY(seqs):\n",
    "    X, Y = zip(*[offset_seq(s) for s in seqs])\n",
    "    return np.array(X, dtype=object), np.array(Y, dtype=object)\n",
    "\n",
    "\n",
    "def seqs_to_npXY(seqs):\n",
    "    X, Y = zip(*[offset_np(s) for s in seqs])\n",
    "    return np.array(X, dtype=object), np.array(Y, dtype=object)\n",
    "\n",
    "\n",
    "def seqs_to_lmnpXY(seqs):\n",
    "    X, Y = zip(*[offset_lm_np(s) for s in seqs])\n",
    "    return np.array(X, dtype=object), np.array(Y, dtype=object)\n",
    "\n",
    "def fraq_loss(vocab, word_to_num, vocabsize):\n",
    "\tfraction_lost = float(sum([vocab['count'][word] for word in vocab.index if (not word in word_to_num) and (not word == \"UNK\")]))\n",
    "\tfraction_lost /= sum([vocab['count'][word] for word in vocab.index if (not word == \"UNK\")])\n",
    "\treturn fraction_lost\n",
    "\n",
    "def adjust_loss(loss, fracloss, q, mode='basic'):\n",
    "\tif mode == 'basic':\n",
    "\t\t# remove freebies only: score if had no UNK\n",
    "\t\treturn (loss + fracloss*np.log(fracloss))/(1 - fracloss)\n",
    "\telse:\n",
    "\t\t# remove freebies, replace with best prediction on remaining\n",
    "\t\treturn loss + fracloss*np.log(fracloss) - fracloss*np.log(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 2000 words from 9954 (88.35% of all tokens)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_size = 10000\n",
    "dev_size = 1000\n",
    "vocab_size = 2000\n",
    "\n",
    "data_folder = \"../data/\"\n",
    "np.random.seed(2018)\n",
    "hdim = 20\n",
    "lookback = 5\n",
    "lr = 0.5\n",
    "\n",
    "# get the data set vocabulary\n",
    "vocab = pd.read_table(data_folder + \"/vocab.wiki.txt\", header=None, sep=\"\\s+\", index_col=0,\n",
    "                        names=['count', 'freq'], )\n",
    "num_to_word = dict(enumerate(vocab.index[:vocab_size]))\n",
    "word_to_num = invert_dict(num_to_word)\n",
    "\n",
    "# calculate loss vocabulary words due to vocab_size\n",
    "fraction_lost = fraq_loss(vocab, word_to_num, vocab_size)\n",
    "print(\n",
    "    \"Retained %d words from %d (%.02f%% of all tokens)\\n\" % (\n",
    "    vocab_size, len(vocab), 100 * (1 - fraction_lost)))\n",
    "\n",
    "# load training data\n",
    "sents = load_np_dataset(data_folder + '/wiki-train.txt')\n",
    "S_train = docs_to_indices(sents, word_to_num, 0, 0)\n",
    "X_train, D_train = seqs_to_npXY(S_train)\n",
    "\n",
    "X_train = X_train[:train_size]\n",
    "Y_train = D_train[:train_size]\n",
    "\n",
    "# load development data\n",
    "sents = load_np_dataset(data_folder + '/wiki-dev.txt')\n",
    "S_dev = docs_to_indices(sents, word_to_num, 0, 0)\n",
    "X_dev, D_dev = seqs_to_npXY(S_dev)\n",
    "\n",
    "X_dev = X_dev[:dev_size]\n",
    "D_dev = D_dev[:dev_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
